# ğŸ“Š Customer Churn ETL Pipeline (Dockerized)

This project demonstrates a modular ELT pipeline using **Apache Airflow**, **PostgreSQL**, **Pandas**, and **Metabase**, containerized using **Docker Compose**. It performs hourly ingestion of customer churn data, processes it for missing values and PII, and exposes it for reporting.

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/                  # Airflow DAG definition
â”‚   â”‚   â””â”€â”€ churn_etl_dag.py
â”‚   â”œâ”€â”€ jars/                  # PostgreSQL JDBC JARs
â”‚   â”œâ”€â”€ Dockerfile             # Custom Airflow image with Python packages
â”‚   â””â”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ data/                      # Raw data files
â”‚   â””â”€â”€ customer_churn_data.csv
â”œâ”€â”€ etl/                       # Python ETL layer (modular)
â”‚   â”œâ”€â”€ extract.py
â”‚   â”œâ”€â”€ transform.py
â”‚   â”œâ”€â”€ load.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ Dashboard/                # Metabase screenshots (for demo)
â”‚   â””â”€â”€ img.png
â”œâ”€â”€ docker-compose.yml        # All service definitions
â””â”€â”€ README.md                 # This file
```

---

## ğŸ’  Tech Stack

- **Airflow** (2.9.1, Python 3.10)
- **PostgreSQL** (15)
- **Pandas** for data transformation
- **Metabase** (reporting)
- **Docker Compose**

---

## ğŸš€ Setup Instructions

### âœ… Prerequisites

- Docker Desktop installed
- Internet connection to pull images

---

### ğŸ§± Build & Start the Containers

```bash
git clone <your-repo-url>
cd hg-insigth-project

# Start everything
docker-compose up --build -d
```

### ğŸ˜ PostgreSQL

- Host: `localhost`
- Port: `5433`
- User: `user`
- Password: `password`
- DB: `staging_db`

### ğŸŒ Airflow

- Open: [http://localhost:8080](http://localhost:8080)

  ```
- Username: `airflow`
- Password: `hginsigth123`
- Trigger DAG: `churn_etl_dag`

### ğŸ“Š Metabase

- Open: [http://localhost:3000](http://localhost:3000)
- Connect to DB:
  - Host: `pg-staging`
  - Port: `5432`
  - User: `user`, Password: `password`
  - DB name: `staging_db`
- Explore the processed data: `processed_customer_data` table

---

## ğŸ”„ DAG Flow Overview

1. **Extract**: Load CSV from `/data/customer_churn_data.csv`
2. **Load raw**: Store into `raw_customer_data` table
3. **Transform**:
   - Fill missing `TotalCharges`, `TechSupport`
   - Fill missing `InternetService` with most common value
   - Anonymize `CustomerID` with SHA-256
4. **Load processed**: Save cleaned data to `processed_customer_data`

Runs **hourly** via Airflow's `@hourly` schedule.

---

## ğŸ“¸ Dashboard



- Go to `Out analytics' from collections
- Selct `Customer Churn` dashboard
The Metabase dashboard shows:

- Churn rate
- Breakdown by contract type and internet service



---

## ğŸ©± Cleanup

```bash
docker-compose down -v
```

---

## ğŸ“Œ Notes

- All data and processing runs inside Docker containers.
- Metabase dashboards are **not persisted** â€” use screenshots or rebuild manually.
- Airflow password is regenerated every container restart. Fetch from logs as shown above.

---

## âœ… Author

Built by Ninad Thorat as part of HG Insight assessment.

